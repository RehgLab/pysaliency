{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18dc5e5a-ece3-477c-9090-b4265ba04942",
   "metadata": {},
   "source": [
    "# Pysaliency: A short tutorial\n",
    "\n",
    "`pysaliency` is a python library which aims at making analyzing and modeling of eye movement data convenient. It was build and extended over the course of multiple papers which are reflected in its structure, mainly:\n",
    "\n",
    "* [Kümmerer, Wallis & Bethge: Information-theoretic model comparison unifies saliency metrics. PNAS 2015](http://www.pnas.org/content/112/52/16054)\n",
    "* [Kümmerer, Wallis & Bethge: Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics. ECCV 2018](http://openaccess.thecvf.com/content_ECCV_2018/html/Matthias_Kummerer_Saliency_Benchmarking_Made_ECCV_2018_paper.html)\n",
    "* [Kümmerer & Bethge: Predicting Visual Fixations, Annual Reviews in Vision Science 2023](https://www.annualreviews.org/doi/10.1146/annurev-vision-120822-072528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c68bf6-8d6d-4c13-934d-8555bf7ca985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pysaliency\n",
    "\n",
    "'''\n",
    "MATLAB is needed to run the various processing scripts/model scripts\n",
    "These were writen in MATLAB originally and the authors built Python \n",
    "wrappers to call them.\n",
    "'''\n",
    "os.environ['PATH'] = f'/software/matlab-R2023b/bin/:{os.environ[\"PATH\"]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8bc27-b407-4844-b381-8bd22e3a8f72",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "`pysaliency` has two main classes for handling data. `pysaliency.Stimuli` contains images which have been shown to a subject, `pysaliency.Fixations` keeps track of recorded fixations. For the purpose of this tutorial, we'll use the MIT1003 dataset, which `pysaliency` can download and import on it's own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ec6a86-bebe-4cdb-b863-1557905c1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stimuli, fixations = pysaliency.get_mit1003(location='pysaliency_datasets')\n",
    "stimuli, fixations = pysaliency.get_cat2000_train(location='pysaliency_datasets', version='1.1')\n",
    "# stimuli, fixations = pysaliency.external_datasets.coco_freeview.get_COCO_Freeview_train(location='pysaliency_datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31fae21-1642-4041-9cd7-76b4a2ef5298",
   "metadata": {},
   "source": [
    "`Stimuli` hold the images in `Stimuli.stimuli` as numpy array. In the case of large datasets, the subclass `FileStimuli` (which is used here) will make sure that the images are only loaded once they are needed.\n",
    "\n",
    "`Stimuli` also contain a list of filenames. Will be useful if we want to create torch datasets later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bd172-bb86-4e23-897e-0c6d60042de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_index in range(10):\n",
    "    print(f\"{image_index:02d}: {stimuli.stimuli[image_index].shape} ({stimuli.filenames[image_index]})\")\n",
    "\n",
    "print(f\"Total number of stimuli in dataset: {len(stimuli)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a4f72",
   "metadata": {},
   "source": [
    "Plotting the stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51a9e1-0cfa-4258-8ae5-195e45ec5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4, 4, figsize=(12, 8))\n",
    "\n",
    "for k, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(stimuli.stimuli[k])\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee2535-3201-4d83-b5e3-1d33d53dc4f0",
   "metadata": {},
   "source": [
    "`Stimuli` instances implement the `collections.abs.Sequence` interface and hence behave mostly like normal lists, e.g. you can slice them with `stimuli[10:20]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b18d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_stimuli = stimuli[3:5]\n",
    "for i in range(len(sliced_stimuli)):\n",
    "    print(sliced_stimuli.filenames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfde1a-a30d-4fa0-b396-9934bea9f7df",
   "metadata": {},
   "source": [
    "`Fixations` hold the fixations made on images. Fixations also behave mostly like a list, where each list item is a fixation made on an image. This also\n",
    "holds for nearly all attributes, which are usually numpy arrays with one row per fixation. The most important attributes are `Fixations.x` and `Fixations.y` which\n",
    "contain the x and y positions of the fixations in pixels. `Fixations` are always meant to be used together with a `Stimuli` object, where `Fixations.n` indicates for each fixation on which image it was made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb1ba4-062d-48df-8cdd-4d41167d5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 42\n",
    "\n",
    "plt.imshow(stimuli.stimuli[image_index])\n",
    "\n",
    "fixation_indices = fixations.n == image_index\n",
    "plt.scatter(fixations.x[fixation_indices], fixations.y[fixation_indices], 20, 'red', alpha=0.5)\n",
    "\n",
    "plt.title(\"Fixations on a given image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365fb7b-39e2-42c6-89bb-a75bc536b931",
   "metadata": {},
   "source": [
    "Just like `Stimuli`, `Fixations` support `len`, slicing and indexing. For example, the last cell could have\n",
    "been written more elegantly as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d8e6c-dccc-4c28-bb76-2cea4b9d4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 42\n",
    "image_fixations = fixations[fixations.n == image_index]\n",
    "\n",
    "plt.imshow(stimuli.stimuli[image_index])\n",
    "plt.scatter(image_fixations.x, image_fixations.y, 20, 'red', alpha=0.5)\n",
    "\n",
    "plt.title(\"Fixations on a given image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4f436-05e7-41c9-a6f8-8951ed3a229c",
   "metadata": {},
   "source": [
    "Fixations don't happen independently, they happen in sequences of so called *Scanpaths* and hence can depend on the previous fixations. Because of that,\n",
    "for each fixation in a dataset, `Fixations` makes the previous fixations available via the attributes `Fixations.x_hist` and `Fixations.y_hist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd7dd6-52e3-4490-a62f-a07beaf1aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixation_index = 130\n",
    "\n",
    "print(f\"x, y position: ({fixations.x[fixation_index]}, {fixations.y[fixation_index]})\")\n",
    "print(f\"number of previous fixations: {fixations.scanpath_history_length[fixation_index]}\")\n",
    "print(f\"previous x locations: {fixations.x_hist[fixation_index]}\")\n",
    "print(f\"previous y locations: {fixations.y_hist[fixation_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a03586-ef62-422c-88f7-e12acecef95c",
   "metadata": {},
   "source": [
    "`pysaliency.plotting` contains some functions to make visualizing a fixation with its history easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217590e-8b70-48aa-b543-b271417dea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysaliency.plotting import plot_scanpath\n",
    "\n",
    "plt.imshow(stimuli.stimuli[fixations.n[fixation_index]])\n",
    "plot_scanpath(stimuli, fixations, fixation_index, visualize_next_saccade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b5455-63a0-4edc-92a0-c5da4a9e6960",
   "metadata": {},
   "source": [
    "## ScanpathFixations\n",
    "\n",
    "While for modeling, it often makes sense to think about each fixation separately, together with their respective history of previous fixations, when building datasets and doing analysis, it's also convenient to think about whole scanpaths. This is what `ScanpathFixations` is for: it's a `Fixations` subclass for fixations which come from scanpaths. Actually, the `fixations` object we worked with so far is such a case: It has an additional attribute `scanpaths` which holds a `Scanpaths` instance containing the actual scanpaths from which the Fixations instance has been build by concatenating the fixations from all scanpaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa9787-11b0-4564-8954-047efecc5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanpaths = fixations.scanpaths\n",
    "print(f\"The dataset has {len(fixations)} fixations coming from a total of {len(scanpaths)} scanpaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixations.x[:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in fixations.x_hist[:23]: print(x)\n",
    "print('-'*100)\n",
    "print(scanpaths.xs[0])\n",
    "print(scanpaths.xs[1])\n",
    "print(scanpaths.xs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scanpaths.subject[:100])\n",
    "print(scanpaths.n[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545624-dfad-4f99-a5ed-4fa1aaf9fcf1",
   "metadata": {},
   "source": [
    "`Scanpaths` as attributes `xs` and `ys` with the x and y locations of all fixations in a scanpath, and `length` containing the length of each scanpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d2df8-9cd5-414c-89df-3693446cdd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = np.random.RandomState(seed=23)\n",
    "scanpath_indices = rst.randint(len(fixations.scanpaths), size=3)\n",
    "\n",
    "for scanpath_index in scanpath_indices:\n",
    "    print(f\"Scanpath no {scanpath_index}:\")\n",
    "    scanpath_length = scanpaths.length[scanpath_index]\n",
    "    xs = scanpaths.xs[scanpath_index]\n",
    "    ys = scanpaths.ys[scanpath_index]\n",
    "    for k, (x, y) in enumerate(zip(xs, ys)):\n",
    "        print(f\"{k}: ({x:.01f}, {y:.01f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b0db8-f770-47fb-ad23-a1f6dbc88deb",
   "metadata": {},
   "source": [
    "In a `ScanpathFixations` instance, where the fixations come from scanpaths, the fixations have an additional attribute `scanpath_index`, which allows to got back from individual fixations to the scanpaths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5b246-16bb-46a3-b600-c06203b0e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanpath_index = scanpath_indices[0]\n",
    "fixation_indices = fixations.scanpath_index == scanpath_index\n",
    "\n",
    "print(fixations.x[fixation_indices])\n",
    "print(fixations.y[fixation_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d304a-6328-4a61-8cba-82f7583fb7f8",
   "metadata": {},
   "source": [
    "### Filtering fixations\n",
    "\n",
    "One might ask: why separating between `ScanpathFixations` and `Fixations` in the first place? After all, fixations always come from scanpaths.\n",
    "The main reason is there are many case where we are only interested in some fixations, but need to be aware of the full scanpath\n",
    "history for each of those fixations. One very important case is that in most experiments, the first fixation is not a voluntary fixation, but a forced central fixation.\n",
    "There is no point in modeling and predicting this fixation, or including it in evaluating models. But of course when predicting the later, voluntary fixations, models need to be aware of the initial central fixation. In this case, we can easily filter a `ScanpathFixations` instance (or a `Fixations` instance) accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a92d74-4512-42c7-b21f-e62e211df361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter fixations to exclude initial fixations from each scanpath\n",
    "voluntary_fixations = fixations[fixations.scanpath_history_length > 0]\n",
    "\n",
    "# show first three fixations\n",
    "for fixation_index in range(3):\n",
    "    x_pos = voluntary_fixations.x[fixation_index]\n",
    "    x_hist = voluntary_fixations.x_hist[fixation_index]\n",
    "    print(f\"{fixation_index}: {x_pos:.01f}, previous x positions: {x_hist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8378b5-69da-4f82-a4a9-2bc5a3fd84b0",
   "metadata": {},
   "source": [
    "We can see that the original initial fixation at x=502.3 is included in the history of all subsequent fixations, but it's not a fixation in the `Fixations` instance on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983438de-18e0-46c8-a220-852e495ead7b",
   "metadata": {},
   "source": [
    "Similarly, we can filter `Fixations` and `ScanpathFixations` instances according to other attribute. One important attribute is `subject`, which encodes\n",
    "the id of the subject which made a certain fixation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2085fb4-6a30-426a-8a47-0da876d92ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fixations.subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733938e-8d5b-472f-8478-3312522bc059",
   "metadata": {},
   "source": [
    "With this attribute, we can easily select all fixations from any given subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a06b0-f339-41d7-9f74-bba4ee9bebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 5\n",
    "\n",
    "subject_fixations = fixations[fixations.subject == subject_id]\n",
    "\n",
    "image_index = 42\n",
    "image_fixations = fixations[fixations.n == image_index]\n",
    "image_subject_fixations = subject_fixations[subject_fixations.n == image_index]\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].imshow(stimuli.stimuli[image_index])\n",
    "axs[0].scatter(image_fixations.x, image_fixations.y, 20, 'red', alpha=0.5)\n",
    "axs[0].set_title(\"all fixations\");\n",
    "\n",
    "axs[1].imshow(stimuli.stimuli[image_index])\n",
    "axs[1].scatter(image_subject_fixations.x, image_subject_fixations.y, 20, 'red', alpha=0.5)\n",
    "axs[1].set_title(f\"fixations from subject {subject_id}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef971db-ed8e-4e49-92e8-2f4d918def01",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "`Fixations` instances can have more attributes besides `x`, `y`, `t`, `{x,y,t}_hist` and `subject`. We can check `Fixations.__attributes__` to see all available attributes (this will probably change in a future version a bit). In the case of the MIT1003 dataset, we also have information about fixation durations and the duration of previous fixations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8068b3c-c1ca-4a64-adae-a7ac769e3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all attributes:\", fixations.__attributes__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce329c1e-9b74-45fe-9d94-ee1d37e067ad",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Pysaliency's main modeling framework is that of probabilistic models, where a model predicts fixations via the means of a probability distribtion (see, e.g. Kümmerer & Bethge 2023). For predicting spatial fixation densities, pysaliency uses the class `Model`, which needs to implement a function `_log_density` for computing a predicted log density. This is an example for a simple model, which predicts fixations to be distributed according to a central Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106515c4-3f4c-43f6-8d58-501045132c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleModel(pysaliency.Model):\n",
    "    def __init__(self, width=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.width = width\n",
    "\n",
    "    def _log_density(self, stimulus: Union[pysaliency.datasets.Stimulus, np.ndarray]):\n",
    "        # _log_density can either take pysaliency Stimulus objects, or, for convenience, simply numpy arrays\n",
    "        # `as_stimulus` ensures that we have a Stimulus object\n",
    "        stimulus_object = pysaliency.datasets.as_stimulus(stimulus)\n",
    "\n",
    "        # size contains the height and width of the image, but not potential color channels\n",
    "        height, width = stimulus_object.size\n",
    "\n",
    "        xs = np.arange(width, dtype=float)\n",
    "        ys = np.arange(height, dtype=float)\n",
    "        XS, YS = np.meshgrid(xs, ys)\n",
    "\n",
    "        XS -= 0.5 * width\n",
    "        YS -= 0.5 * height\n",
    "\n",
    "        max_size = max(width, height)\n",
    "        actual_kernel_size = self.width * max_size\n",
    "\n",
    "        gaussian = np.exp(-0.5 * (XS ** 2 + YS ** 2) / actual_kernel_size ** 2)\n",
    "        \n",
    "        density = gaussian / gaussian.sum()\n",
    "        return np.log(density)\n",
    "\n",
    "my_simple_model = MySimpleModel(width=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f8c0a-b98b-4e43-8867-6a82c8f68f2d",
   "metadata": {},
   "source": [
    "When using the model, we use the function `log_density`, which mainly adds a cache around `_log_density` to avoid recomputing log densities multiple times. This is how the resulting prediction looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbe076-72d3-4441-a095-69a455e2a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysaliency.plotting import visualize_distribution\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(8, 5))\n",
    "\n",
    "image_index = 0\n",
    "\n",
    "axs[0].imshow(stimuli.stimuli[image_index])\n",
    "axs[0].set_axis_off()\n",
    "axs[0].set_title(\"Image\")\n",
    "\n",
    "axs[1].matshow(my_simple_model.log_density(stimuli[image_index]))\n",
    "axs[1].set_axis_off()\n",
    "axs[1].set_title(\"model log density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4e1f9-7a0b-476a-9a41-41e97561245b",
   "metadata": {},
   "source": [
    "pysaliency comes with a range of fixation models for comparision, for example [DeepGaze I](http://arxiv.org/abs/1411.1045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6dad7-e6a9-4a66-b924-8dbc3312ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepgaze needs a spatial prior, for which we use our Gaussian model here\n",
    "deepgaze1_model = pysaliency.external_models.DeepGazeI(centerbias_model=my_simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8141d94-eb1e-44ca-81b5-43ae30a4c5ab",
   "metadata": {},
   "source": [
    "Because visualizing densities is nontrivial, `pysaliency.plotting` contains the function `visualize_distribution`\n",
    "to get a nice visualization (for details check Figure 5 in the appendix of https://arxiv.org/abs/1704.08615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e64ed-ff99-4932-98f8-35cabf60c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysaliency.plotting import visualize_distribution\n",
    "\n",
    "f, axs = plt.subplots(1, 3, figsize=(12, 8))\n",
    "\n",
    "image_index = 0\n",
    "\n",
    "axs[0].imshow(stimuli.stimuli[image_index])\n",
    "axs[0].set_axis_off()\n",
    "axs[0].set_title(\"Image\")\n",
    "\n",
    "axs[1].matshow(deepgaze1_model.log_density(stimuli[image_index]))\n",
    "axs[1].set_axis_off()\n",
    "axs[1].set_title(\"model log density\")\n",
    "\n",
    "visualize_distribution(deepgaze1_model.log_density(stimuli[image_index]))\n",
    "axs[2].set_axis_off()\n",
    "axs[2].set_title(\"model prediction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e827eba-9389-4e51-a581-7f509447877d",
   "metadata": {},
   "source": [
    "Probabilistic models allow for straight forward sampling of new fixations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ad889-dbeb-41d6-b79d-1adb0b23150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 120 new fixations (or, more precisely, scanpaths of length 1):\n",
    "rst = np.random.RandomState(42)  # pysaliency allows to specify the random state for deterministic behaviour\n",
    "actual_fixations = fixations[fixations.n == 0]\n",
    "new_fixations_gaussian = my_simple_model.sample(stimuli[:1], train_counts=120, lengths=1, rst=rst)\n",
    "new_fixations_deepgaze = deepgaze1_model.sample(stimuli[:1], train_counts=120, lengths=1, rst=rst)\n",
    "\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "axs[0, 0].imshow(stimuli.stimuli[0])\n",
    "axs[0, 0].scatter(actual_fixations.x, actual_fixations.y, 20, 'red', alpha=0.5)\n",
    "axs[0, 0].set_title(\"Real fixations\")\n",
    "\n",
    "visualize_distribution(deepgaze1_model.log_density(stimuli[0]), ax=axs[0, 1])\n",
    "axs[0, 1].set_title(\"DeepGaze I prediction\")\n",
    "axs[0, 2].imshow(stimuli.stimuli[0])\n",
    "axs[0, 2].scatter(new_fixations_deepgaze.x, new_fixations_deepgaze.y, 20, 'red', alpha=0.5)\n",
    "axs[0, 2].set_title(\"DeepGaze I fixations\")\n",
    "\n",
    "visualize_distribution(my_simple_model.log_density(stimuli[0]), ax=axs[1, 1])\n",
    "axs[1, 2].imshow(stimuli.stimuli[0])\n",
    "axs[1, 1].set_title(\"Gaussian prediction\")\n",
    "axs[1, 2].scatter(new_fixations_gaussian.x, new_fixations_gaussian.y, 20, 'red', alpha=0.5)\n",
    "axs[1, 2].set_title(\"Gaussian fixations\")\n",
    "\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb13b89-44c4-42d6-b0bd-331142f11d3b",
   "metadata": {},
   "source": [
    "## Scanpath models\n",
    "\n",
    "As already mentioned, fixation usually depend on previous fixation locations. Scanpath models aim at incorporating these dependencies. As discussed in [Kümmerer & Bethge: Predicting Visual Fixations, Annual Reviews in Vision Science 2023](https://www.annualreviews.org/doi/10.1146/annurev-vision-120822-072528) in detail, \"next-fixation-prediction\" is a powerful way to unify many different gaze prediction settings including scanpath prediction and spatial density prediction. The key idea is to not model whole scanpaths at once, but instead for each fixation in a scanpath predict a probability distribution of possible next fixation locations given the previous fixations, that is:\n",
    "\n",
    "$$\n",
    "  p(x_{i+1}, y_{i+1} \\mid x_0, y_0, \\dots, x_{i}, y_{i}, I)\n",
    "$$\n",
    "\n",
    "In the case of spatial gaze density prediction as above, e.g. using DeepGaze I, the dependency on previous fixations is not used and the model prediction for the next fixation is simply the predicted gaze density:\n",
    "\n",
    "$$\n",
    "  p(x_{i+1}, y_{i+1} \\mid x_0, y_0, \\dots, x_{i}, y_{i}, I) \\stackrel{\\text{density prediction}}{=} p(x, y \\mid I)\n",
    "$$\n",
    "\n",
    "Pysaliency provides the class `ScanpathModel` for modeling scanpaths. Instead of implementing `_log_density`, now we need to implement `conditional_log_density(stimulus, x_hist, y_hist)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696744b-6cf7-4c43-8fe8-93a6f1a0fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysaliency.utils import remove_trailing_nans\n",
    "\n",
    "class MySimpleScanpathModel(pysaliency.ScanpathModel):\n",
    "    def __init__(self, prior_width: float=0.3, saccade_width: float=0.2):\n",
    "        self.prior_width = prior_width\n",
    "        self.saccade_width = saccade_width\n",
    "\n",
    "    def conditional_log_density(self, stimulus, x_hist, y_hist, t_hist, attributes=None, out=None,):\n",
    "        stimulus_object = pysaliency.datasets.as_stimulus(stimulus)\n",
    "\n",
    "        # size contains the height and width of the image, but not potential color channels\n",
    "        height, width = stimulus_object.size\n",
    "\n",
    "        # compute prior\n",
    "            \n",
    "        xs = np.arange(width, dtype=float)\n",
    "        ys = np.arange(height, dtype=float)\n",
    "        XS, YS = np.meshgrid(xs, ys)\n",
    "\n",
    "        XS -= 0.5 * width\n",
    "        YS -= 0.5 * height\n",
    "\n",
    "        max_size = max(width, height)\n",
    "        actual_kernel_size = self.prior_width * max_size\n",
    "\n",
    "        prior_gaussian = np.exp(-0.5 * (XS ** 2 + YS ** 2) / actual_kernel_size ** 2)\n",
    "\n",
    "        # compute saccade bias\n",
    "\n",
    "        last_x = x_hist[-1]\n",
    "        last_y = y_hist[-1]\n",
    "        \n",
    "        xs = np.arange(width, dtype=float)\n",
    "        ys = np.arange(height, dtype=float)\n",
    "        XS, YS = np.meshgrid(xs, ys)\n",
    "\n",
    "        XS -= last_x\n",
    "        YS -= last_y\n",
    "\n",
    "        max_size = max(width, height)\n",
    "        actual_kernel_size = self.saccade_width * max_size\n",
    "\n",
    "        saccade_bias = np.exp(-0.5 * (XS ** 2 + YS ** 2) / actual_kernel_size ** 2)\n",
    "\n",
    "        prediction = prior_gaussian * saccade_bias\n",
    "        \n",
    "        density = prediction / prediction.sum()\n",
    "        return np.log(density)\n",
    "\n",
    "my_simple_scanpath_model = MySimpleScanpathModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f010b8c-d368-4c5c-b241-1ebf88219b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixation_index = 130\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].imshow(stimuli.stimuli[fixations.n[fixation_index]])\n",
    "plot_scanpath(stimuli, fixations, fixation_index, visualize_next_saccade=True, ax=axs[0])\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "prediction = my_simple_scanpath_model.conditional_log_density(\n",
    "    stimuli.stimuli[fixations.n[fixation_index]],\n",
    "    x_hist=fixations.x_hist[fixation_index],\n",
    "    y_hist=fixations.y_hist[fixation_index],\n",
    "    t_hist=None,\n",
    ")\n",
    "\n",
    "visualize_distribution(prediction, ax=axs[1])\n",
    "plot_scanpath(stimuli, fixations, fixation_index, visualize_next_saccade=True, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880840df-56c8-4275-b241-9eaa8f0a8cfa",
   "metadata": {},
   "source": [
    "Since computing the conditional log density for a given fixation in a dataset is a very common task, `ScanpathModel` provides\n",
    "the convenience method `conditional_log_density_for_fixation(stimuli, fixations, fixation_index)` to that end. Using it, we\n",
    "could also have written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fe712-34fa-4201-8415-8542c0080ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = my_simple_scanpath_model.conditional_log_density_for_fixation(\n",
    "    stimuli,\n",
    "    fixations,\n",
    "    fixation_index=fixation_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e49392-d81d-47ac-be1b-27db3eebf7ef",
   "metadata": {},
   "source": [
    "## Evaluating models\n",
    "\n",
    "\n",
    "Pysaliency incorporates extensive mechanisms for evaluationg model performances. For probabilistic models, i.e., instances of `pysaliency.Model` and `pysaliency.ScanpathModel`, pysaliency makes it simple to compute log-likelihood and information gain scores ([Kümmerer et al, PNAS 2015](http://www.pnas.org/content/112/52/16054), [Kümmerer & Bethge, Ann.Rev.Vis.Sci 2023](https://www.annualreviews.org/doi/10.1146/annurev-vision-120822-072528)). Other popular metrics like AUC, CC etc can also be computed as we'll see below by using the methods from [Kümmerer et al, ECCV 2018](http://openaccess.thecvf.com/content_ECCV_2018/html/Matthias_Kummerer_Saliency_Benchmarking_Made_ECCV_2018_paper.html).\n",
    "\n",
    "Information gain is the difference in log-likelihood between a model and a baseline model:\n",
    "\n",
    "$$\n",
    "  IG(\\hat p, p_\\text{baseline}) = \\log \\hat p(x_i, y_i) - \\log p_\\text{baseline}(x_i, y_i)\n",
    "$$\n",
    "\n",
    "The model method `information_gains` computes information gain values for each fixation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38220bad-2512-465d-8d90-f4bfc7426667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to exclude the initial fixation from evaluation\n",
    "eval_fixations = fixations[fixations.scanpath_history_length > 0]\n",
    "\n",
    "# we only want to evaluate the first 20 fixations\n",
    "eval_fixations = eval_fixations[:20]\n",
    "deepgaze1_model.information_gains(stimuli, eval_fixations, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99e67d-9238-45db-9f71-8603c90edb1b",
   "metadata": {},
   "source": [
    "By default, `information_gains` uses a uniform baseline model, but we can hand over any other model. Often, it makes sense\n",
    "to use a center bias model as prior, in which case the name \"information gain\" is actually justified.\n",
    "\n",
    "Often we're only interested in average performance over a full dataset. In this case, we can use the method `information_gain` instead of `information_gains`\n",
    "which takes care of the averaging. Since we want each image to contribute equally to the score, we'll use `average='image'`. By default each fixation contributes equally (`average='fixations')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e032691-527e-456e-8639-7f360e4582e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"information gain relative to a uniform baseline model   \", deepgaze1_model.information_gain(stimuli, eval_fixations, average='image'))\n",
    "print(\"information gain relative to a centerbias baseline model\", deepgaze1_model.information_gain(stimuli, eval_fixations, baseline_model=my_simple_model, average='image'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48ff16-f17a-4838-9015-90b0834656d9",
   "metadata": {},
   "source": [
    "One advantage of the framework of next-fixation-prediction is that it allows easy comparison of spatial models and scanpath models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b8e2e-24a6-44c0-9992-e37336317962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple Scanpath Model: IG =\", my_simple_scanpath_model.information_gain(stimuli, eval_fixations, verbose=False, average='image'))\n",
    "print(\"DeepGaze I:            IG =\", deepgaze1_model.information_gain(stimuli, eval_fixations, verbose=False, average='image'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfd978-9932-44df-ae87-f3545aa945a2",
   "metadata": {},
   "source": [
    "We can see that in this case the better understanding of image-based effects of DeepGaze I outweights the additional dynamics of the simple scanpath model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13b917-fc3f-4dd9-9a2f-b62835085ff1",
   "metadata": {},
   "source": [
    "## Saliency Map Models\n",
    "\n",
    "Traditionally, the field of fixation prediction mainly formulated their models as so called *saliency models*. Saliency models predict fixation locations by the means of a *saliency map*, where areas of high saliency are expected to have more fixations. The reason for this somewhat vague definition has historical reasons, see [Kümmerer & Bethge, Ann.Rev.Vis.Sci 2023](https://www.annualreviews.org/doi/10.1146/annurev-vision-120822-072528)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e00a8-be31-4529-a50a-2a5e928a93fa",
   "metadata": {},
   "source": [
    "Pysaliency uses the class `pysaliency.SaliencyMapModel` to implement saliency map models. They behave very similarly to the `Model` class, but instead of methods `log_density` and `_log_density`, they have methods `saliency_map` and `_saliency_map` with identical signature. Pysaliency comes with a range of published saliency models prewrapped, we'll use the [AIM](https://jov.arvojournals.org/article.aspx?articleid=2193531) model here as an example. Most saliency models are implemented in matlab and hence require matlab to run. Pysaliency will automatically download the original source code, potentially apply some patches to make it run in more modern matlab versions and then call matlab as part of the `_saliency_map` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255672f-e3df-4b1d-8cad-cb8e7a39fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_model = pysaliency.AIM(location='pysaliency_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c137e6-0fa0-4f19-bf92-75e93ed6bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "image_index = 0\n",
    "\n",
    "axs[0].imshow(stimuli.stimuli[image_index])\n",
    "axs[0].set_axis_off()\n",
    "axs[0].set_title(\"Image\")\n",
    "\n",
    "axs[1].matshow(aim_model.saliency_map(stimuli[image_index]))\n",
    "axs[1].set_axis_off()\n",
    "axs[1].set_title(\"AIM saliency map\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66259030-fd15-422c-b6fe-aa1dba6d73ad",
   "metadata": {},
   "source": [
    "Saliency map models don't allow information theoretic evaluation with information gain. Instead, a multitude of different saliency metrics has been proposed. Pysaliency implements many of the common metrics, such as AUC, sAUC, NSS, CC, SIM and KL-Div as methods of `pysaliency.SaliencyMapModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d068c-96c2-410b-8a62-fc229ad45453",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUC: {aim_model.AUC(stimuli, eval_fixations, average='image'):.04f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ceb085-6943-45ec-9b6a-29a8262fb03e",
   "metadata": {},
   "source": [
    "### Saliency metric score for probabilistic models\n",
    "\n",
    "`Model` and `ScanpathModel` don't support saliency metrics as AUC and CC directly, because it's not clear what to use as saliency map for the metric\n",
    "and indeed for different metrics different saliency maps are optimal ([Kümmerer et al, ECCV 2018](http://openaccess.thecvf.com/content_ECCV_2018/html/Matthias_Kummerer_Saliency_Benchmarking_Made_ECCV_2018_paper.html)). Before computing saliency metrics, probabilistic models have to be converted to saliency map models. For AUC, we can simply use predicted fixation density as saliency map (but for other metrics, much more complicated maps might be appropriate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c10abe-57a2-4bbc-937f-74488a2636e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepgaze1_density_map_model = pysaliency.DensitySaliencyMapModel(deepgaze1_model)\n",
    "print(f\"DeepGaze AUC: {deepgaze1_density_map_model.AUC(stimuli, eval_fixations, average='image'):.04f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61804a56-ed7c-4aa8-a2ef-2684a0abb018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
